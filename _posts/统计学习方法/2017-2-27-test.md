---
layout: post
category : statistical learning
tagline: ""
tags : [statistical learning]
---


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. 逻辑斯蒂回归</a>
<ul>
<li><a href="#sec-1-1">1.1. 逻辑斯蒂分布</a></li>
<li><a href="#sec-1-2">1.2. 二项逻辑斯蒂回归模型</a></li>
<li><a href="#sec-1-3">1.3. 模型参数估计</a></li>
<li><a href="#sec-1-4">1.4. 多项logistic regression</a></li>
</ul>
</li>
<li><a href="#sec-2">2. 最大熵模型</a>
<ul>
<li><a href="#sec-2-1">2.1. 最大熵原理</a></li>
<li><a href="#sec-2-2">2.2. 最大熵模型的定义</a></li>
<li><a href="#sec-2-3">2.3. 最大熵模型的学习</a></li>
<li><a href="#sec-2-4">2.4. 极大似然估计</a></li>
</ul>
</li>
<li><a href="#sec-3">3. 模型学习的最优化算法</a>
<ul>
<li><a href="#sec-3-1">3.1. 改进的迭代尺度法(IIS)</a></li>
<li><a href="#sec-3-2">3.2. 拟牛顿法</a></li>
</ul>
</li>
</ul>
</div>
</div>

逻辑斯蒂回归与最大熵模型都属于对数线性模型

# 逻辑斯蒂回归<a id="sec-1" name="sec-1"></a>

## 逻辑斯蒂分布<a id="sec-1-1" name="sec-1-1"></a>

## 二项逻辑斯蒂回归模型<a id="sec-1-2" name="sec-1-2"></a>

定义：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_63a8cd3f76856d47c08ce58207e8bceb413f5087.png" alt="$$ P(y = 1| x) = \frac {exp(\vec w \cdot \vec x + b)} {1 + exp( \vec 
w \cdot \vec x + b)}$$" />
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_582e4589d2d379775170e7aa343d456989af8bd7.png" alt="$$P(y=0|x) = \frac {1} {1 + exp(\vec w \cdot \vec x + b)}$$" />
比较两个条件概率的大小，将实例x分到条件概率值较大的那一类。
对数几率或logit函数是：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_5e9ff12c59e480f78ec354f6ef068f65a2ba39e5.png" alt="$$logit(p) = log \frac {p}{1-p}$$" />
在logistic regression模型中，输出 Y=1 的对数几率是输入x的线性函数
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_e0935b11453070bd70d05d3886c36ffbf77b890c.png" alt="$$logit(P(Y=1|x)) = \vec w \cdot \vec x$$" />

## 模型参数估计<a id="sec-1-3" name="sec-1-3"></a>

应用极大似然估计法估计模型的参数
设: <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_2535e24fb9d0397e4021da4aa1ace0bbd06c4388.png" alt="$P(Y=1|x) = \pi(x), P(Y=0|x) = 1 - \pi(x)$" />
似然函数为：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_3c1b78599d9d1f241469acb62601f1b4be125004.png" alt="$$ \prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$" />
对数似然函数为：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_b3f4a59cfdb4846e7414e4bae532b8e2bfb3ad19.png" alt="$$L(\vec w) = \sum_{i=1}^{N}[y_i(\vec w \cdot \vec {x_i}) -log(1+exp(\vec w \cdot \vec {x_i}))]$$" />
对对数似然函数求极大值，得到 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_f49a09d6e5847a2d5aaaaad009c181209d663272.png" alt="$\vec w$" /> 的估计值。
问题变成以对数似然函数为目标函数的最优化问题。
通常采用梯度下降法和拟牛顿法。

## 多项logistic regression<a id="sec-1-4" name="sec-1-4"></a>

# 最大熵模型<a id="sec-2" name="sec-2"></a>

由最大熵推导实现

## 最大熵原理<a id="sec-2-1" name="sec-2-1"></a>

假设离散随机变量X的概率分布为P(X)，则其熵为：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_2e3cde0f87d5a1a961f020fe79cd6d95e3cb1869.png" alt="$$H(P)= - \sum_xP(x)logP(x)$$" /> 
熵最大的模型为最好的模型。
最大熵原理通过熵的最大化来表示等可能性。

## 最大熵模型的定义<a id="sec-2-2" name="sec-2-2"></a>

对于给定的输入X,以条件概率P(Y|X)输出Y。
学习的目标:用最大熵原理选择最好的分类模型。
最大熵模型
  定义在条件概率分布P(Y|X)上的条件熵为：
    <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_16aa1399688b5a5cf37793e1b5dcc487ff664ed7.png" alt="$$H(P) = -\sum_{x,y}P(x)P(y|x)log(y|x) $$" /> 
则满足所有约束条件的模型集合为C中条件熵H(P)最大的模型称为最大熵模型。

## 最大熵模型的学习<a id="sec-2-3" name="sec-2-3"></a>

形式化为约束最优化问题，将约束最优化问题转换为无约束最优化的对偶问题，通过解对偶问题求解原问题。
最大熵模型：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_131305828b55c18e30eefb856b1dc007f39b7d60.png" alt="$$P_w(y|x) = \frac{1}{Z_w(x)} exp(\sum_{i=1}^nw_if_i(x,y))$$" />
其中，
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_8bd2e25b1d3859ab8cf24887eccdf365d720c95f.png" alt="$$Z_w(x) = \sum_yexp(\sum_{i=1}^nw_if_i(x,y))$$" />
特征函数为 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_2ad6f492ed32007d191c88145c6544e2e7252250.png" alt="$f_i(x,y)$" /> ， <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_eed52a4961904f7fd51fdff8f5a1040e74cf6a0a.png" alt="$Z_{\vec w}(x)$" /> 称为规范化因子， n为特征函数的个数， <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_bae1ec9a8ea6fc39f4cfc01682f3f87d3da01824.png" alt="$w_i$" /> 为特征的权值。

## 极大似然估计<a id="sec-2-4" name="sec-2-4"></a>

证明了最大熵模型学习中的对偶函数的极大化等价于最大熵模型的极大似然估计
最大熵模型的一般形式
  <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_7b5801941c79166ce717e55357af5c13fcb71633.png" alt="$$P_w(y|x) = \frac1{Z_w(x)}exp(\sum_{i=1}^n w_i f_i(x,y))$$" />
其中
  <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_8d879e50293888db99ff540bba99d32f1b32ae28.png" alt="$$ Z_w(x) = \sum_yexp(\sum_{i=1}^nw_if_i(x,y)) $$" />
  这里 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_65fa73aa012ebdbcf8e5713edd194829cc5d95d4.png" alt="$\vec x\in R^n$" /> 为输入， <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_92fb0e8e699808dfbfa95a8a124cd9cddc65e485.png" alt="$y\in{1,2,...,K}$" /> 为输出， <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_4837025268f1afcb02a25ff27b7c96f67ce27b63.png" alt="$\vec w \in R^n$" /> 为权值向量，<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_abeffe67509a9b639c4e0be8f2a5320e92dfec99.png" alt="$f_i(x,y), i=1,2,...,n$" /> 为任意实值特征函数。

# 模型学习的最优化算法<a id="sec-3" name="sec-3"></a>

logistic regression、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代法求解。能保证找到全局最优解，牛顿法和拟牛顿法收敛速度快。

## 改进的迭代尺度法(IIS)<a id="sec-3-1" name="sec-3-1"></a>

当前的参数向量是 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_6a3dc6544369a3bcc40dc2f19833c4da104c97a5.png" alt="$w$" />,找到一个新的参数向量 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_b7185ac7daef0bf508a38c50df428119b05adbdc.png" alt="$w+\delta$" />,使得模型的对数似然函数增大，重复这一过程，直至找到对数似然函数的最大值。

## 拟牛顿法<a id="sec-3-2" name="sec-3-2"></a>

最大熵模型：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_1ff6583c49b081f897ff976eabdc23b191b7f0f0.png" alt="$$P_{\vec w}(y|x) = \frac{exp(\sum{{i=1}^nw_if_i(x,y)})}{\sum_yexp(\sum_{i=1}^nw_if_i(x,y))}$$" />
目标函数（熵最大化）：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_dddaff6f75beb13981abe571b83e0896c7a113ad.png" alt="$$min_{\vec w \in R^n} f(\vec 
w) = ... $$" />
梯度：
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_e6c1a88e7839c6c2235508b3fa842a6612d30bcb.png" alt="$$g(\vec w) = (\frac {\partial f(\vec w)}{\partial w_1}, \frac {\partial f(\vec w)}{\partial w_2},...,\frac {\partial f(\vec w)}{\partial w_n})^T $$" />

算法：
(1)选定初始点 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_d5cb0af938b937b1f5566aab816db3232efcd273.png" alt="$w^{(0)}$" /> ,取 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_03768afd59b66406a6c6c6b4471141601ce92f50.png" alt="$B_0$" /> 为正定对称矩阵，置k=0
(2)计算 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_a3bc7cc8f49fe791e14748c1eafc75f34797e4db.png" alt="$g_k = g(w^{(k)})$" /> , 若 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_b03f096187ae0893b1e09f090ea1bdde69e9d671.png" alt="$||g_k|| &amp;lt; \epsilon$" /> ,则停止计算， 得 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_f659547a290ce6f18c550bc9faf39fd8b65cf4a1.png" alt="$w = w^{(k)}$" /> ; 否则转到(3)
(3)由 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_19593fceabc52e0dbdc3809fa27e7389cfe462e5.png" alt="$B_kp_k = -g_k$" /> 求出 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_29aa6945c785d67f37d0f305a536a0c734a71d8f.png" alt="$p_k$" /> 。
(4)一维搜索：求 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_dd252e7887eaafc054a53653dfced6a680719907.png" alt="$\lambda_k$" /> 使得
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_4370875cc47b07c49f9cf7e37bd28b05b2977c40.png" alt="$$f(w^{(k)} + \lambda_kp_k) =
\min_{\lambda \ge 0} f(w^{(k)} + \lambda p_k)$$" />
(5)置 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_7a178e177633ac986d53c57b54b847bb807cd610.png" alt="$$w^{(k+1)} = w^{(k)} + \lambda_kp_k $$" />
(6)计算 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_239a9b2f4afda38a0cb446dee91bca1f34f405b0.png" alt="$$g_{k+1} = g({w^(k+1)}) $$" /> , 若 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_0e4808840190a88158e12ccba46747b55503847e.png" alt="$$||g_{k+1}|| &amp;lt; \epsilon$$" /> , 则停止计算, 得 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_66808653780d3c91008fa1afc492fbea518c7462.png" alt="$$w = w^{(k+1)}$$" /> ; 否则， 按下式求出 <img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_4806fe9c7ce8ad16115a308dd48ea7e7ac78c27e.png" alt="$$B_{k+1}$$" /> :
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_7cf7af119168d6ab21ae620791c36d10e267afdc.png" alt="$$B_{k+1} = B_k + \frac {y_ky_k^T}{y_k^T\lambda_k} - \frac{B_k\lambda_k\lambda_k^TB_k}{\lambda_k^TB_k\lambda_k
}$$" />
其中，
<img src="ltxpng/2017-2-27-逻辑斯蒂回归与最大熵模_f23e8926d23e18c35ad93437333e745a3951fc83.png" alt="$$y_k = g_{k+1} - g_k, \lambda_k = w^{(k+1)} - w^{(k)}$$" />
(7)置 k = k +1, 转(3).

