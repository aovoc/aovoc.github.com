
# Table of Contents

1.  [线性可分支持向量机和硬间隔最大化](#org27f0bed)
    1.  [线性可分支持向量机](#org980f0bb)
    2.  [函数间隔和几何间隔](#orgeafba70)
    3.  [间隔最大化](#org3bdbf58)
        1.  [最大间隔分离超平面](#org4e8a741)
        2.  [支持向量和间隔边界](#org3da9ba0)
        3.  [学习的对偶算法](#org2b86260)
2.  [线性支持向量机与软间隔最大化](#orgf420949)
    1.  [线性支持向量机](#orgbceeb29)
    2.  [学习的对偶算法](#orgd12c101)
    3.  [支持向量](#orgf681033)
    4.  [合页损失函数](#org11248cc)
3.  [非线性支持向量机和核函数](#orgbd90db6)
    1.  [核技巧](#org61bddf2)
    2.  [正定核](#org1d0f222)
    3.  [常用核函数](#org8736797)
    4.  [非线性支持向量分类机](#orgb281b9b)
4.  [序列最小最优化算法](#orga9dd2bf)
    1.  [两个变量二次规划的求解方法](#org1d7239e)
    2.  [变量选择的方法](#org9019346)

在特征空间间隔最大大的线性分类器。
核技巧，非线性分类器
学习策略：间隔最大化m
学习算法是求解凸二次规划的最优化算法

模型：线性可分支持向量机、线性支持向量机、非线性支持向量机（由简到繁）
训练数据线性可分 硬间隔最大化 线性可分支持向量机
训练数据近似可分  软间隔最大化  线性支持向量机 
训练数据线性不可分  核技巧和软间隔最大化  非线性支持向量机

输入空间是欧式空间或者离散集合、特征空间是希尔伯特空间时，核函数：将输入从输入空间映射到特征空间得到特征向量之间的内积。
核方法是比支持向量机更为一般的机器学习方法。


<a id="org27f0bed"></a>

# 线性可分支持向量机和硬间隔最大化


<a id="org980f0bb"></a>

## 线性可分支持向量机

利用间隔最大化求最优分离超平面，解唯一。


<a id="orgeafba70"></a>

## 函数间隔和几何间隔

函数间隔
样本点 <img src="ltximg/2017-2-27-支持向量机_06641331faffdb0a4e8760a85a6d0c89574f04bb.png" alt="2017-2-27-支持向量机_06641331faffdb0a4e8760a85a6d0c89574f04bb.png" /> 的函数间隔：
  <img src="ltximg/2017-2-27-支持向量机_559b642e9d888554d42575a5ab5ae8288367d330.png" alt="2017-2-27-支持向量机_559b642e9d888554d42575a5ab5ae8288367d330.png" />
关于训练集的函数间隔：
<img src="ltximg/2017-2-27-支持向量机_8f9b67efe53f2cd4f570c12c4efb53adedb5ec3b.png" alt="2017-2-27-支持向量机_8f9b67efe53f2cd4f570c12c4efb53adedb5ec3b.png" />
对w进行约束，||w|| = 1,函数间隔变成几何间隔


<a id="org3bdbf58"></a>

## 间隔最大化


<a id="org4e8a741"></a>

### 最大间隔分离超平面

线性可分支持向量机学习的最优化问题：
<img src="ltximg/2017-2-27-支持向量机_2f65496992913f181d0cd625bf0bd0976aeba7b0.png" alt="2017-2-27-支持向量机_2f65496992913f181d0cd625bf0bd0976aeba7b0.png" />
<img src="ltximg/2017-2-27-支持向量机_7c0ec6c18c2ffff85b660f13d18c96170101d77d.png" alt="2017-2-27-支持向量机_7c0ec6c18c2ffff85b660f13d18c96170101d77d.png" />
最大间隔分离超平面的存在具有唯一性


<a id="org3da9ba0"></a>

### 支持向量和间隔边界

训练样本点与分离超平面距离最近的实例为支持向量


<a id="org2b86260"></a>

### 学习的对偶算法

优点：对偶问题往往更容易求解、自然引入核函数，推广到非线性分类
根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
<img src="ltximg/2017-2-27-支持向量机_8077e9c9e260e46abca2b8a12bfb975279ba18dd.png" alt="2017-2-27-支持向量机_8077e9c9e260e46abca2b8a12bfb975279ba18dd.png" />
为了得到对偶问题的解，需要先求 <img src="ltximg/2017-2-27-支持向量机_b2cec238c8109bf32e9d6910ce5cb62ed42c9520.png" alt="2017-2-27-支持向量机_b2cec238c8109bf32e9d6910ce5cb62ed42c9520.png" /> 对 <img src="ltximg/2017-2-27-支持向量机_6aec663a7b5fc0a127e006e60dd936cd3ed09608.png" alt="2017-2-27-支持向量机_6aec663a7b5fc0a127e006e60dd936cd3ed09608.png" />的极小化，接着求对<img src="ltximg/2017-2-27-支持向量机_36c9f1cf2d465863a352295f59745d756d2755d9.png" alt="2017-2-27-支持向量机_36c9f1cf2d465863a352295f59745d756d2755d9.png" />的极大化。
与之等价的对偶最优化问题：
<img src="ltximg/2017-2-27-支持向量机_37dabe1df4096ca2a63dc833d24baa65704f132d.png" alt="2017-2-27-支持向量机_37dabe1df4096ca2a63dc833d24baa65704f132d.png" />
<img src="ltximg/2017-2-27-支持向量机_498620a70976e7db4cde2290f61e7b79287676c7.png" alt="2017-2-27-支持向量机_498620a70976e7db4cde2290f61e7b79287676c7.png" />
分类决策函数只依赖于输入x和训练样本输入的内积

对于给定的线性可分训练集，通过对偶问题求得w,b, 得到分离超平面和分类决策函数。这种方法是线性可分支持向量机的基本学习算法。

w,b只依赖于训练样本中对应于 $&alpha;<sub>i</sub> > 0 $ 的样本点，将这样的点称为支持向量。


<a id="orgf420949"></a>

# 线性支持向量机与软间隔最大化


<a id="orgbceeb29"></a>

## 线性支持向量机

引入松弛变量 <img src="ltximg/2017-2-27-支持向量机_978c0a1eafcf1d96f20e2bfe2802e51f6c29fe37.png" alt="2017-2-27-支持向量机_978c0a1eafcf1d96f20e2bfe2802e51f6c29fe37.png" />,使函数间隔加上松弛变量大于等于0，约束条件变为：
<img src="ltximg/2017-2-27-支持向量机_8b783a7cbf9575fbe2b1a1aafa3aaa8c179f1579.png" alt="2017-2-27-支持向量机_8b783a7cbf9575fbe2b1a1aafa3aaa8c179f1579.png" />
目标函数变为：
<img src="ltximg/2017-2-27-支持向量机_bc4dd45fdedc9a5554fcea053ffb2cc9d68befaa.png" alt="2017-2-27-支持向量机_bc4dd45fdedc9a5554fcea053ffb2cc9d68befaa.png" />
C > 0称为惩罚参数
使 <img src="ltximg/2017-2-27-支持向量机_28f6a9d10c1da6fd17c6ee33726bcd8fc98cc608.png" alt="2017-2-27-支持向量机_28f6a9d10c1da6fd17c6ee33726bcd8fc98cc608.png" /> 尽量小即间隔尽量大，使误分类的额个数尽量小
线性不可分的线性支持向量机学习的原始问题：
<img src="ltximg/2017-2-27-支持向量机_b30ef9b05dc0f5f8bfbb8c7c358c29ed1e9250ff.png" alt="2017-2-27-支持向量机_b30ef9b05dc0f5f8bfbb8c7c358c29ed1e9250ff.png" />
<img src="ltximg/2017-2-27-支持向量机_a8d7dab134b616bcc85853bbdcfbedbd7838ff9a.png" alt="2017-2-27-支持向量机_a8d7dab134b616bcc85853bbdcfbedbd7838ff9a.png" />
<img src="ltximg/2017-2-27-支持向量机_602d1e28601cf5b4ff1635d6e940f5db616ce408.png" alt="2017-2-27-支持向量机_602d1e28601cf5b4ff1635d6e940f5db616ce408.png" />
可以证明w的解是唯一的，b的解不唯一，存在一个区间


<a id="orgd12c101"></a>

## 学习的对偶算法

原始问题对应的对偶问题不变，满足的条件发生改变 $ 0<= &alpha;<sub>i</sub> <= C, i=1,2,&#x2026;N$


<a id="orgf681033"></a>

## 支持向量

软间隔的支持向量或者在间隔的边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分类一侧。


<a id="org11248cc"></a>

## 合页损失函数

线性支持向量机学习，模型：分离超平面及决策函数，学习策略：软间隔最大化，学习算法：凸二次规划
另一种解释，最小化如下目标函数：
<img src="ltximg/2017-2-27-支持向量机_83557b19037795b72f07e94e4281b3aedf718c53.png" alt="2017-2-27-支持向量机_83557b19037795b72f07e94e4281b3aedf718c53.png" />
目标函数的第一项称为经验损失，第一项称为合页损失函数，第二项是正则化项。


<a id="orgbd90db6"></a>

# 非线性支持向量机和核函数


<a id="org61bddf2"></a>

## 核技巧

核技巧通过一个非线性变换将输入空间对应与一个特征空间。分类任务在特征空间求解线性支持向量机。
存在输入空间到特征空间的映射
<img src="ltximg/2017-2-27-支持向量机_d9ed44c1610ed37daed21280d67702dce2a93254.png" alt="2017-2-27-支持向量机_d9ed44c1610ed37daed21280d67702dce2a93254.png" /> 
使得对所有的 <img src="ltximg/2017-2-27-支持向量机_369cb860fae4fad95f7b85b4ab1a2e5f5e460458.png" alt="2017-2-27-支持向量机_369cb860fae4fad95f7b85b4ab1a2e5f5e460458.png" /> ,函数K(x,z)满足条件
<img src="ltximg/2017-2-27-支持向量机_3ca3fdce302f1310a11f37340d2fdf1c40282a35.png" alt="2017-2-27-支持向量机_3ca3fdce302f1310a11f37340d2fdf1c40282a35.png" />
则称K(x,z)为核函数(任意两个输入变量在高维映射中的内积)， <img src="ltximg/2017-2-27-支持向量机_2fbc620054b0ff515a7017aaf8518b24b8ee6a88.png" alt="2017-2-27-支持向量机_2fbc620054b0ff515a7017aaf8518b24b8ee6a88.png" /> 为映射函数。
核技巧：只定义核函数，而不显示地定义映射函数。直接计算核函数容易，而通过映射函数计算复杂。
对于给定的核函数，特征空间和映射函数大的取法不唯一。

核技巧在支持向量机中的应用
在对偶的目标函数中的内积 <img src="ltximg/2017-2-27-支持向量机_a2373338fa8171a53759ca14621b684e848676c6.png" alt="2017-2-27-支持向量机_a2373338fa8171a53759ca14621b684e848676c6.png" /> 可以用核函数 <img src="ltximg/2017-2-27-支持向量机_a6770573de71a948cc75d392deb4fe867079047f.png" alt="2017-2-27-支持向量机_a6770573de71a948cc75d392deb4fe867079047f.png" /> 来代替，同时分类决策函数中的内积也可以用核函数来代替。


<a id="org1d0f222"></a>

## 正定核

K(x,z)为对称函数，则K(x,z)为正定核函数的充要条件是对任意的 <img src="ltximg/2017-2-27-支持向量机_35aaa76d2e72b779acf94a700591d60f57d4b151.png" alt="2017-2-27-支持向量机_35aaa76d2e72b779acf94a700591d60f57d4b151.png" /> K(x,z)对应的Gram矩阵：
<img src="ltximg/2017-2-27-支持向量机_7e29e571fd19c8065bc4c5715cfc5dc52063306c.png" alt="2017-2-27-支持向量机_7e29e571fd19c8065bc4c5715cfc5dc52063306c.png" />
是半正定矩阵。
检验是否是正定核函数并不容易。


<a id="org8736797"></a>

## 常用核函数

多项式核函、高斯核函数、字符串核函数
字符串核函数：两个字符串相同的子串越多，他们就越相似，字符串核函数的值就越大。可以由动态规划快速计算。


<a id="orgb281b9b"></a>

## 非线性支持向量分类机

将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。

非线性支持向量机学习算法
（1）选取适当的核函数K(x,z)和适当的参数C，构造并求解最优化问题
<img src="ltximg/2017-2-27-支持向量机_e94711550889e309aff6cf25b36d9d0f32a1a50b.png" alt="2017-2-27-支持向量机_e94711550889e309aff6cf25b36d9d0f32a1a50b.png" />
<img src="ltximg/2017-2-27-支持向量机_6aaaaf06104a21f86908edd6468cac770b94a0c6.png" alt="2017-2-27-支持向量机_6aaaaf06104a21f86908edd6468cac770b94a0c6.png" />
<img src="ltximg/2017-2-27-支持向量机_31ae134fdb0e07e886f4137af93fb56dc0207ebc.png" alt="2017-2-27-支持向量机_31ae134fdb0e07e886f4137af93fb56dc0207ebc.png" />
求得最优解 <img src="ltximg/2017-2-27-支持向量机_231cf4839fcc74685657e05a251db207471d292d.png" alt="2017-2-27-支持向量机_231cf4839fcc74685657e05a251db207471d292d.png" />
选取 <img src="ltximg/2017-2-27-支持向量机_123ff64bf05f6b7f3a0eb1734f377bb15651ab29.png" alt="2017-2-27-支持向量机_123ff64bf05f6b7f3a0eb1734f377bb15651ab29.png" /> 求
<img src="ltximg/2017-2-27-支持向量机_1bc907774f5bddc35539a5e8d59a650dff6b234d.png" alt="2017-2-27-支持向量机_1bc907774f5bddc35539a5e8d59a650dff6b234d.png" />
构造决策函数
<img src="ltximg/2017-2-27-支持向量机_e75c6aa9291868b86c4d5d3da58eefa6125a73d2.png" alt="2017-2-27-支持向量机_e75c6aa9291868b86c4d5d3da58eefa6125a73d2.png" />


<a id="orga9dd2bf"></a>

# 序列最小最优化算法

SMO算法：若所有变量的解都满足KKT条件，则解就得到了；否则，选择两个变量，固定其他变量，构建二次规划问题并求解，将原问题不断分解为子问题并对子问题求解，进而求解原问题。
包含：求解两个变量二次规划的解析方法、选择变量的启发式方法


<a id="org1d7239e"></a>

## 两个变量二次规划的求解方法


<a id="org9019346"></a>

## 变量选择的方法

第1个变量的选择
外层循环选取违背KKT条件最严重的点，首先遍历在间隔边界上的支持向量的点，检验是否满足条件，若都满足条件，则遍历整个训练集，检验是否满足条件。
第2个变量的选择
选择的标准，使第二个变量有足够大的变化

