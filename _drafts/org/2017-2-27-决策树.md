
# Table of Contents

1.  [决策树模型与学习](#orgb343105)
    1.  [决策树模型](#org3a72b55)
    2.  [决策树与if-then规则](#orgd1449c2)
    3.  [决策树与条件概率分布](#org92f0826)
    4.  [决策树学习](#org2d8d98b)
2.  [特征选择](#org51e726f)
    1.  [特征选择问题](#org5051c18)
    2.  [信息增益](#org08159e4)
    3.  [信息增益比](#org4f3a51a)
3.  [决策树的生成](#org21fa7da)
    1.  [ID3算法](#orga6fbf68)
    2.  [C4.5的生成算法](#orge5aad64)
4.  [决策树的剪枝](#orge2011c3)
5.  [CART算法](#orge7679d1)
    1.  [CART的生成](#org3a8514d)
    2.  [CART剪枝](#org33c6121)

呈树形，基于特征对实例进行分类的过程。可以认为是if-then 规则的集合，或者在类空间和概率空间的条件概率分布。
通常包含3个步骤：特征选择、决策树的生成、决策树的修剪。


<a id="orgb343105"></a>

# 决策树模型与学习


<a id="org3a72b55"></a>

## 决策树模型

决策树由结点和有向边组成。
结点：内部结点、叶结点
内部结点表示一个特征或属性，叶结点表示一个类。
将实例分到叶结点的类中。


<a id="orgd1449c2"></a>

## 决策树与if-then规则

if-then规则的一个重要的性质：互斥并且完备


<a id="org92f0826"></a>

## 决策树与条件概率分布

决策树还可以表示给定特征条件下类的条件概率分布。
决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。


<a id="org2d8d98b"></a>

## 决策树学习

本质：归纳出一组分类规则。
由训练数据集归纳出条件概率模型，得到的模型应该对未知数据有较好的预测。
决策树学习的损失函数：对数似然损失
策略：以损失函数为目标函数的最小化
启发式学习方法 次最优
递归地选择最优特征，对训练数据进行分割
过拟合 剪枝 泛化能力
学习算法包含：特征选择、决策树的生成、决策树的剪枝
常用算法:ID3、C4.5、CART


<a id="org51e726f"></a>

# 特征选择


<a id="org5051c18"></a>

## 特征选择问题

特征选择的准则：信息增益和信息增益比。


<a id="org08159e4"></a>

## 信息增益

熵：随机变量不确定性的度量
设X是一个取有限个值的离散随机变量，其概率分布为：
<img src="ltximg/2017-2-27-决策树_041f24293b7b0bbf732a7c4d9376417250ac8e15.png" alt="2017-2-27-决策树_041f24293b7b0bbf732a7c4d9376417250ac8e15.png" />
则随机变量X的熵定义为：
<img src="ltximg/2017-2-27-决策树_d4b9650a614f375e28c3e10281e82c1ad81a939c.png" alt="2017-2-27-决策树_d4b9650a614f375e28c3e10281e82c1ad81a939c.png" />
熵只依赖于X的分布，而与X的取值无关，可将X的熵记为H(p)
熵越大，随机变量的不确定性越大

条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。

当熵和条件熵中的概率由数据估计得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。

信息增益表示得知特征X的信息而得到类Y的不确定性减少的程度。
定义为集合D的经验熵H(D)与特征A给定的条件下D的经验条件熵H(D|A)之差(互信息)：
  <img src="ltximg/2017-2-27-决策树_26f30d0cf3d3c1ef50650af20f8da83c255913cc.png" alt="2017-2-27-决策树_26f30d0cf3d3c1ef50650af20f8da83c255913cc.png" />
决策树学习应用信息增益准则来选取特征，表示在特征A给定的条件下对数据集D进行分类的不确定性。
特征选择方法：计算各特征的信息增益，选择信息增益最大的特征。


<a id="org4f3a51a"></a>

## 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比进行校正。
信息增益比 <img src="ltximg/2017-2-27-决策树_10445678d0a9813ba868decad122f274b97fa47d.png" alt="2017-2-27-决策树_10445678d0a9813ba868decad122f274b97fa47d.png" /> 定义为信息增益 <img src="ltximg/2017-2-27-决策树_ab2256b4db499108f049723c3d98caffc9bd27df.png" alt="2017-2-27-决策树_ab2256b4db499108f049723c3d98caffc9bd27df.png" /> 与训练数据集关于特征A的值的熵 <img src="ltximg/2017-2-27-决策树_447e4dd719a832a0d1b99a2e3aa67a2c691808ea.png" alt="2017-2-27-决策树_447e4dd719a832a0d1b99a2e3aa67a2c691808ea.png" /> 之比。其中， <img src="ltximg/2017-2-27-决策树_6e68810bfaf60ee190a5339b85c71a6b877053ce.png" alt="2017-2-27-决策树_6e68810bfaf60ee190a5339b85c71a6b877053ce.png" /> 。


<a id="org21fa7da"></a>

# 决策树的生成


<a id="orga6fbf68"></a>

## ID3算法

核心：在决策树各个结点上应用信息增益准则选取特征，递归得构建决策树。
ID3相当于用极大似然法进行概率模型的选择。
只有树的生成，容易产生过拟合。


<a id="orge5aad64"></a>

## C4.5的生成算法

对ID3进行改进，用信息增益比选择特征。


<a id="orge2011c3"></a>

# 决策树的剪枝

通过极小化决策树整体的损失函数或代价函数来实现。
决策树的损失函数可以定义为：
  <img src="ltximg/2017-2-27-决策树_07fa0fe8f67668bd89638490dc40e5b6385543a5.png" alt="2017-2-27-决策树_07fa0fe8f67668bd89638490dc40e5b6385543a5.png" />
其中经验熵：
  <img src="ltximg/2017-2-27-决策树_3876201b5a5befdf7fa16eab241cea3003d57822.png" alt="2017-2-27-决策树_3876201b5a5befdf7fa16eab241cea3003d57822.png" />
  <img src="ltximg/2017-2-27-决策树_60c84728a9febe85f5af2f2382a1b5500727be7b.png" alt="2017-2-27-决策树_60c84728a9febe85f5af2f2382a1b5500727be7b.png" /> : 叶结点有 <img src="ltximg/2017-2-27-决策树_60c84728a9febe85f5af2f2382a1b5500727be7b.png" alt="2017-2-27-决策树_60c84728a9febe85f5af2f2382a1b5500727be7b.png" /> 个样本点。
树T的叶结点个数为|T|
损失函数的极小化等价于正则化的极大似然估计

树的剪枝算法
  递归地从叶结点向上回缩，若损失函数减小，则进行剪枝。
  计算可在局部进行，可由动态规划算法实现。


<a id="orge7679d1"></a>

# CART算法

CART 分类与回归树
CART:在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。
包含两个步骤：决策树的生成、决策树剪枝


<a id="org3a8514d"></a>

## CART的生成

回归树：平方误差最小化
分类树：基尼系数最小化

回归树的生成
假设将输入空间划分为M个单元，每个单元上有一个固定的输出值<img src="ltximg/2017-2-27-决策树_ae381398bf16c77a77b6832258535f28861edd81.png" alt="2017-2-27-决策树_ae381398bf16c77a77b6832258535f28861edd81.png" />,于是回归模型可表示为：
<img src="ltximg/2017-2-27-决策树_1d0b015eddd48bed7495ab0bc8b21a28f2d7f3f3.png" alt="2017-2-27-决策树_1d0b015eddd48bed7495ab0bc8b21a28f2d7f3f3.png" />
当输入空间划分确定时，可以用平方误差 <img src="ltximg/2017-2-27-决策树_9cfe2b5fc1e7e416b9e18d49c2ace086fe95b2b4.png" alt="2017-2-27-决策树_9cfe2b5fc1e7e416b9e18d49c2ace086fe95b2b4.png" /> 来表示回归树对于训练数据的预测误差，用平方误差最小原则求解每个单元的最优输出值。
单元 <img src="ltximg/2017-2-27-决策树_ce9af233aee92c58dd2661cf06439ace14fcebde.png" alt="2017-2-27-决策树_ce9af233aee92c58dd2661cf06439ace14fcebde.png" /> 上的 <img src="ltximg/2017-2-27-决策树_ae381398bf16c77a77b6832258535f28861edd81.png" alt="2017-2-27-决策树_ae381398bf16c77a77b6832258535f28861edd81.png" /> 的最优值 <img src="ltximg/2017-2-27-决策树_76c4e28e101531a19a0c2773d6d2af3fc89f831d.png" alt="2017-2-27-决策树_76c4e28e101531a19a0c2773d6d2af3fc89f831d.png" /> 是 <img src="ltximg/2017-2-27-决策树_ce9af233aee92c58dd2661cf06439ace14fcebde.png" alt="2017-2-27-决策树_ce9af233aee92c58dd2661cf06439ace14fcebde.png" /> 上的所有输入实例 <img src="ltximg/2017-2-27-决策树_c7757f6fab91f9a49d7e8b923e19c0e9d2d2d951.png" alt="2017-2-27-决策树_c7757f6fab91f9a49d7e8b923e19c0e9d2d2d951.png" /> 输出的均值。
<img src="ltximg/2017-2-27-决策树_9ffe697165c838ad5e2e0c0d0daa33cb12f7b355.png" alt="2017-2-27-决策树_9ffe697165c838ad5e2e0c0d0daa33cb12f7b355.png" />
对输入空间进行划分，启发式：
选择第j个变量 <img src="ltximg/2017-2-27-决策树_92b50a292cabc9dcb086dc7d8f71cd31fe8fac57.png" alt="2017-2-27-决策树_92b50a292cabc9dcb086dc7d8f71cd31fe8fac57.png" /> 和它取的值s,作为切分变量和切分点，寻找最优的切分变量和切分点，将输入空间划分为两个区域，对每个子区域重复上述过程，直至满足条件。

分类树的生成
定义基尼指数：
<img src="ltximg/2017-2-27-决策树_47f2157b2961621b5faf8591df19687a53de773b.png" alt="2017-2-27-决策树_47f2157b2961621b5faf8591df19687a53de773b.png" />
对于给定的样本集合D,其基尼指数为
<img src="ltximg/2017-2-27-决策树_5a31aa7a33433ba64cde5cd5f8a118468bc29d00.png" alt="2017-2-27-决策树_5a31aa7a33433ba64cde5cd5f8a118468bc29d00.png" />
<img src="ltximg/2017-2-27-决策树_bd9a5830dfaea38af701c8e1e1238699a78485c2.png" alt="2017-2-27-决策树_bd9a5830dfaea38af701c8e1e1238699a78485c2.png" /> 是D中属于第k类的样本子集， K是类的个数。
若样本集合根据特征A是否取a被划分为 <img src="ltximg/2017-2-27-决策树_4801c53c85c8839a8653d1a96c710048745525b3.png" alt="2017-2-27-决策树_4801c53c85c8839a8653d1a96c710048745525b3.png" /> 和 <img src="ltximg/2017-2-27-决策树_91a236f4e678ab468afe3e899da1c85003f00c64.png" alt="2017-2-27-决策树_91a236f4e678ab468afe3e899da1c85003f00c64.png" /> ，则在特征条件A下，集合D的基尼指数定义为：
<img src="ltximg/2017-2-27-决策树_017ce2832bf3c2027818291f2fa887214685a63a.png" alt="2017-2-27-决策树_017ce2832bf3c2027818291f2fa887214685a63a.png" />
基尼指数表示集合的不确定性，基尼指数越大，集合的不确定性越大。
基尼指数和熵之半的曲线很接近，都可以近似地代表分类的误差率。

CART生成算法
对所有特征的每个取值计算基尼指数，选择基尼指数最小的特征及其所对应的切分点作为最优特征和最优切分点，将训练数据分为两个子结点中。重复直至满足条件。


<a id="org33c6121"></a>

## CART剪枝

从完全生长的决策树底端剪去一些子树。
由两步组成:
  从底端开始不断剪枝，直到根结点，形成子树序列；
  通过交叉验证法在独立的验证集上对子序列进行测试，从中选取最优子树。

1.剪枝，形成一个子树序列
  <img src="ltximg/2017-2-27-决策树_e25d6a35e2f86790a57930d0da6dbe958739cf4b.png" alt="2017-2-27-决策树_e25d6a35e2f86790a57930d0da6dbe958739cf4b.png" />
  <img src="ltximg/2017-2-27-决策树_6d4e8a9e0b8a5bb0f177e969f78933884dbd0d3d.png" alt="2017-2-27-决策树_6d4e8a9e0b8a5bb0f177e969f78933884dbd0d3d.png" /> 为对训练数据的预测误差(如基尼指数)， <img src="ltximg/2017-2-27-决策树_4916c4cf173f815aa198299f3d52c84cedd6b199.png" alt="2017-2-27-决策树_4916c4cf173f815aa198299f3d52c84cedd6b199.png" /> 为子树的叶结点的个数， <img src="ltximg/2017-2-27-决策树_f06f20dee81ab8248628631211abcbe3c7ee9a80.png" alt="2017-2-27-决策树_f06f20dee81ab8248628631211abcbe3c7ee9a80.png" /> 为权衡训练数据拟合程度和模型复杂度，<img src="ltximg/2017-2-27-决策树_aaf6ce0a87df002be31d0e7e8c2225917d33a2ad.png" alt="2017-2-27-决策树_aaf6ce0a87df002be31d0e7e8c2225917d33a2ad.png" /> 为参数为 <img src="ltximg/2017-2-27-决策树_f06f20dee81ab8248628631211abcbe3c7ee9a80.png" alt="2017-2-27-决策树_f06f20dee81ab8248628631211abcbe3c7ee9a80.png" /> 时的子树T的整体损失。 
  用递归的方法进行剪枝，将 <img src="ltximg/2017-2-27-决策树_f06f20dee81ab8248628631211abcbe3c7ee9a80.png" alt="2017-2-27-决策树_f06f20dee81ab8248628631211abcbe3c7ee9a80.png" /> 从小增大，得到临界点的 <img src="ltximg/2017-2-27-决策树_3f81ed7f09456cd50a14ee8e86d3eafe9f5ff930.png" alt="2017-2-27-决策树_3f81ed7f09456cd50a14ee8e86d3eafe9f5ff930.png" /> 的值， <img src="ltximg/2017-2-27-决策树_187b09d9e1825d76ad3a4d448fd867fa96d2f013.png" alt="2017-2-27-决策树_187b09d9e1825d76ad3a4d448fd867fa96d2f013.png" />, 产生一系列的区间 <img src="ltximg/2017-2-27-决策树_70ed1cd43c4d0a6cf8f5bdb4df61ea2c7f63a8c4.png" alt="2017-2-27-决策树_70ed1cd43c4d0a6cf8f5bdb4df61ea2c7f63a8c4.png" />, 对应的最优子树序列 <img src="ltximg/2017-2-27-决策树_993456aaadecd17d864553e51e44159e75118591.png" alt="2017-2-27-决策树_993456aaadecd17d864553e51e44159e75118591.png" /> 。
2.在剪枝得到的子树序列张通过交叉验证选取最优子树 <img src="ltximg/2017-2-27-决策树_595e6e3de328bf92029264e481c113c1c9deb4c4.png" alt="2017-2-27-决策树_595e6e3de328bf92029264e481c113c1c9deb4c4.png" />
  利用独立数据集，测试子树序列的平方误差或基尼系数，最小的决策树被认为是最优的决策树。

